{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "import sklearn \n",
    "import keras.backend\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Read files\n",
    "def read_data(file_name):\n",
    "    df = pd.read_csv(file_name, comment = ';')\n",
    "    df[\"time\"] = pd.date_range(pd.to_datetime('2021-04-14'+' '+df.time[0]),periods = df.shape[0], freq='0.01086S')\n",
    "    df.set_index('time', inplace = True)\n",
    "    df.index = df.index.values.astype('M8[ms]')\n",
    "    df = df.resample(\"10L\", base = 4).first().interpolate()\n",
    "    df = df.rename(columns = {\"index\":\"time\"})\n",
    "    return df\n",
    "def read_esense(file):\n",
    "    df = pd.read_csv(file,header=None)\n",
    "    df.columns = ['time','device','accx','accy','accz','acc1x','acc1y','acc1z','label']\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='ms') + pd.Timedelta(hours = 2)\n",
    "    df.index = df['time']\n",
    "    df = df.resample(\"10L\", base = 4).first().interpolate().drop(columns = [\"time\",\"device\",\"label\"])\n",
    "    return df \n",
    "\n",
    "\n",
    "# For subject2 ankle\n",
    "dribble_sub2_ankle = read_data('dribbling_sbj2_ankle.csv')\n",
    "layup_sub2_ankle = read_data('layup_sbj2_ankle.csv')\n",
    "movements_sub2_ankle_1 = read_data('movements_sbj2_ankle_1.csv')\n",
    "shooting_sub2_ankle_1 = read_data('shooting_sbj2_ankle_1.csv')\n",
    "df_sub2_ankle = pd.concat([shooting_sub2_ankle_1,layup_sub2_ankle,dribble_sub2_ankle,movements_sub2_ankle_1])\n",
    "df_sub2_ankle\n",
    "\n",
    "\n",
    "#For subject1 ankle\n",
    "layup_sub1_ankle = read_data('layup_sbj1_ankle.csv')\n",
    "movements_sub1_ankle_1 = read_data('movements_sbj1_ankle_1.csv')\n",
    "shooting_sub1_ankle = read_data('shooting_sbj1_ankle.csv')\n",
    "df_sub1_ankle = pd.concat([shooting_sub1_ankle,layup_sub1_ankle,movements_sub1_ankle_1])\n",
    "df_sub1_ankle\n",
    "\n",
    "\n",
    "# For esense subject 2\n",
    "df_esense_sub2 = read_esense('eSense_sbj2.csv')\n",
    "df_esense_sub2\n",
    "\n",
    "\n",
    "# For esense subject 1\n",
    "df_esense_sub1 = read_esense('eSense_sbj1.csv')\n",
    "df_esense_sub1\n",
    "\n",
    "\n",
    "# For subject2 wrist\n",
    "dribble_sub2_wrist = read_data('dribbling_sbj2_wrist.csv')\n",
    "layup_sub2_wrist = read_data('layup_sbj2_wrist.csv')\n",
    "movements_sub2_wrist = read_data('movements_sbj2_wrist.csv')\n",
    "shooting_sub2_wrist = read_data('shooting_sbj2_wrist.csv')\n",
    "df_sub2_wrist = pd.concat([shooting_sub2_wrist,layup_sub2_wrist,dribble_sub2_wrist,movements_sub2_wrist])\n",
    "df_sub2_wrist\n",
    "\n",
    "\n",
    "#For subject1 wrist\n",
    "layup_sub1_wrist = read_data('layup_sbj1_wrist.csv')\n",
    "movements_sub1_wrist = read_data('movements_sbj1_wrist.csv')\n",
    "shooting_sub1_wrist = read_data('shooting_sbj1_wrist.csv')\n",
    "df_sub1_wrist = pd.concat([shooting_sub1_wrist,layup_sub1_wrist,movements_sub1_wrist])\n",
    "df_sub1_wrist\n",
    "\n",
    "\n",
    "# data for subject 2\n",
    "df_sub2 = pd.merge(pd.merge(df_esense_sub2,df_sub2_ankle,left_index = True,right_index = True),df_sub2_wrist,left_index = True, right_index = True)\n",
    "df_sub2 = df_sub2.rename(columns = {'acc_x_x':'ankle_x','acc_y_x':'ankle_y','acc_z_x':'ankle_z','accx':'esense_x','accy':'esense_y','accz':'esense_z','acc1x':'gyro_x','acc1y':'gyro_y','acc1z':'gyro_z','acc_x_y':'wrist_x','acc_y_y':'wrist_y','acc_z_y':'wrist_z'})\n",
    "df_sub2\n",
    "\n",
    "\n",
    "# data for subject 1\n",
    "df_sub1 = pd.merge(pd.merge(df_esense_sub1,df_sub1_ankle,left_index = True,right_index = True),df_sub1_wrist,left_index = True,right_index = True)\n",
    "df_sub1 = df_sub1.rename(columns = {'acc_x_x':'ankle_x','acc_y_x':'ankle_y','acc_z_x':'ankle_z','accx':'esense_x','accy':'esense_y','accz':'esense_z','acc1x':'gyro_x','acc1y':'gyro_y','acc1z':'gyro_z','acc_x_y':'wrist_x','acc_y_y':'wrist_y','acc_z_y':'wrist_z'})\n",
    "df_sub1\n",
    "\n",
    "\n",
    "# Labelling\n",
    "column = ['esense_x','esense_y','esense_z','gyro_x','gyro_y','gyro_z','ankle_x','ankle_y','ankle_z','wrist_x','wrist_y','wrist_z']\n",
    "df_sub2[column] = (df_sub2[column]-df_sub2[column].mean())/df_sub2[column].std()\n",
    "df_sub1[column] = (df_sub1[column]-df_sub1[column].mean())/df_sub1[column].std()\n",
    "time1 = [df_sub1.index.isin(df_sub1.between_time('18:33:28', '18:35:44').index), \n",
    "        df_sub1.index.isin(df_sub1.between_time('18:23:24', '18:25:21').index),\n",
    "        df_sub1.index.isin(df_sub1.between_time('18:45:18', '18:47:10').index),\n",
    "        df_sub1.index.isin(df_sub1.between_time('18:13:32', '18:15:40').index),\n",
    "        df_sub1.index.isin(df_sub1.between_time('18:42:49', '18:45:17').index)]\n",
    "subject_1 = ['dribbling', 'layup', 'running', 'shooting', 'walking']\n",
    "df_sub1['Label'] = np.select(time1, subject_1, 'Null')\n",
    "df_sub1 = df_sub1[['esense_x','esense_y','esense_z','gyro_x','gyro_y','gyro_z','ankle_x','ankle_y','ankle_z','wrist_x','wrist_y','wrist_z','Label']]\n",
    "\n",
    "time2 = [df_sub2.index.isin(df_sub2.between_time('19:27:02', '19:29:55').index), \n",
    "        df_sub2.index.isin(df_sub2.between_time('19:17:43', '19:20:07').index),\n",
    "        df_sub2.index.isin(df_sub2.between_time('19:39:46', '19:41:50').index),\n",
    "        df_sub2.index.isin(df_sub2.between_time('19:00:20', '19:02:03').index),\n",
    "        df_sub2.index.isin(df_sub2.between_time('19:37:19', '19:39:46').index)]\n",
    "subject_2 = ['dribbling', 'layup', 'running', 'shooting', 'walking']\n",
    "df_sub2['Label'] = np.select(time2, subject_2, 'Null')\n",
    "df_sub2 = df_sub2[['esense_x','esense_y','esense_z','gyro_x','gyro_y','gyro_z','ankle_x','ankle_y','ankle_z','wrist_x','wrist_y','wrist_z','Label']]\n",
    "df = pd.concat([df_sub1,df_sub2]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Label Encoding\n",
    "le = preprocessing.LabelEncoder()\n",
    "label = le.fit_transform(df[\"Label\"])\n",
    "df[\"Label\"] = label\n",
    "\n",
    "\n",
    "# Splitting X and Y data\n",
    "y_data = df['Label']\n",
    "x_data = df.drop('Label',axis=1)\n",
    "\n",
    "\n",
    "# Sliding window technique\n",
    "def sliding_window(dataset,window_length,overlap_ratio):\n",
    "    #Create empty lists\n",
    "    windows = []\n",
    "    indices = []\n",
    "    non_overlap_elements = 0\n",
    "    if overlap_ratio != None:\n",
    "        overlap_elements = int((overlap_ratio/100)*(window_length))\n",
    "    if overlap_elements >= window_length:\n",
    "        print(\"Overlapping elements are more\")\n",
    "    while(non_overlap_elements < dataset.shape[0] - window_length):\n",
    "        windows.append(dataset.iloc[non_overlap_elements:non_overlap_elements+window_length])\n",
    "        indices.append([non_overlap_elements,non_overlap_elements+window_length])\n",
    "        #Update non_overlap_elements\n",
    "        non_overlap_elements = non_overlap_elements + window_length - overlap_elements\n",
    "        try:\n",
    "            final_windows = np.array(windows)\n",
    "            final_indices = np.array(indices)\n",
    "        except:\n",
    "            final_windows = np.empty( shape =(len(windows), window_length, dataset.shape[1]), dtype =object)\n",
    "            final_indices = np.array(indices)\n",
    "            for i in range(0,len(windows)):\n",
    "                final_windows[i] = windows[i]\n",
    "                final_indices[i] = indices[i]\n",
    "    return final_windows\n",
    "\n",
    "\n",
    "# X_train, y_train, X_test, y_test\n",
    "# Splitting the data into 6 folds. One of them is used for testing among these 6 folds each time\n",
    "# Splitting is done into 6 folds equally and excluding the reamaining data\n",
    "# array_data_x is representation of all the windows x_data that is without label column\n",
    "# array_data_y is representation of all the windows y_data that is the label column\n",
    "# X_train is the split of train data from array_data_x\n",
    "# X_test is the split of test data from array_data_x\n",
    "# y_train is the split of train data from array_data_y\n",
    "# y_test is the split of test data from array_data_y\n",
    "window_length = 100\n",
    "s_x = sliding_window(x_data,window_length,50)\n",
    "s_y = sliding_window(y_data,window_length,50)\n",
    "lists = []\n",
    "b_x = np.asarray(s_x, dtype=np.float32)\n",
    "b_y = np.asarray(s_y, dtype=np.float32)\n",
    "t_x = torch.from_numpy(b_x)\n",
    "t_y = torch.from_numpy(b_y)\n",
    "length_x = int(len(t_x)%6)\n",
    "length_y = int(len(t_y)%6)\n",
    "data_x = t_x[0:len(s_x)-length_x]\n",
    "data_y = t_y[0:len(s_y)-length_y]\n",
    "final_data_x = np.array_split(data_x,6)\n",
    "final_data_y = np.array_split(data_y,6)\n",
    "array_data_x = np.array([np.array(xi) for xi in final_data_x])\n",
    "array_data_y = np.array([np.array(yi) for yi in final_data_y])\n",
    "X_train = array_data_x[0:5]\n",
    "X_test = array_data_x[5:]\n",
    "s_y = torch.tensor(stats.mode(s_y,axis=1).mode)\n",
    "y_train = np.vstack(array_data_y[0:5])\n",
    "y_train = torch.tensor(stats.mode(y_train,axis=1).mode)\n",
    "y_test = np.vstack(array_data_y[5:])\n",
    "y_test = torch.tensor(stats.mode(y_test,axis=1).mode)\n",
    "array_data_x = torch.from_numpy(array_data_x)\n",
    "array_data_x = array_data_x.reshape(array_data_x.shape[0]*array_data_x.shape[1],array_data_x.shape[2],array_data_x.shape[3])\n",
    "array_data_y = np.vstack(array_data_y)\n",
    "array_data_y = torch.tensor(stats.mode(array_data_y,axis=1).mode)\n",
    "array_data_y = array_data_y.reshape(array_data_y.shape[0],)\n",
    "s_y = s_y.reshape(s_y.shape[0],)\n",
    "\n",
    "\n",
    "# Network with 2 conv1D layers and 3 fully connected layers.\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(SimpleCNN,self).__init__()\n",
    "        in_1 = 12\n",
    "        out_1 = 32\n",
    "        out_2 = 64\n",
    "        self.conv1 = nn.Conv1d(in_1,out_1,kernel_size=5)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2,stride=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_1)\n",
    "        self.conv2 = nn.Conv1d(out_1,out_2,kernel_size=5)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2,stride=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_2)\n",
    "        self.fc1 = nn.Linear(5760,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,6)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = x.view(543,5760)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        yhat = torch.FloatTensor(x)\n",
    "        return yhat\n",
    "\n",
    "\n",
    "# Calling SimpleCNN and running on GPU\n",
    "net = SimpleCNN()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(net.parameters(),lr = 0.001)\n",
    "\n",
    "\n",
    "# Applying kfold cross validation to get shuffling between windows and sending different train, test data as 6fold each time.\n",
    "cv = KFold(n_splits=6, random_state=5, shuffle=True)\n",
    "i = 0\n",
    "j = 0\n",
    "correct = []\n",
    "loss =[]\n",
    "test_loss_list = []\n",
    "train_loss_list = []\n",
    "train_correct_list = []\n",
    "test_correct_list = []\n",
    "for train_index, test_index in cv.split(array_data_x):\n",
    "    full_loss = 0\n",
    "    full_correct = 0\n",
    "    X_train, X_test = array_data_x[train_index], array_data_x[test_index]\n",
    "    y_train, y_test = array_data_y[train_index], array_data_y[test_index]\n",
    "    X_train = X_train.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    X_test = X_test.reshape(X_test.shape[0],X_test.shape[2],X_test.shape[1])\n",
    "    y_train = np.array_split(y_train,5)\n",
    "    y_train = np.array([np.array(yi) for yi in y_train]) \n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    train_loader = DataLoader(X_train,int(X_train.shape[0]/5))\n",
    "    total_test_correct = 0\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    # Number of epochs applied on training and testing data\n",
    "    # total_train_loss is train loss after each epoch\n",
    "    # total_test_loss is test loss after each epoch\n",
    "    # total_train_correct is train accuracy after each epoch\n",
    "    # total_test_correct is test accuracy after each epoch\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        correct = 0\n",
    "        for batch in train_loader:\n",
    "            net.train()\n",
    "            batch = batch.reshape(batch.shape[0],batch.shape[2],batch.shape[1])\n",
    "            preds = net(batch)\n",
    "            loss = criterion(preds,y_train[i].long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            total_loss += loss.item()\n",
    "            preds_softmax = F.softmax(preds, dim = 1)\n",
    "            _, preds_tags = torch.max(preds_softmax, dim = 1)\n",
    "            correct_pred = (preds_tags == y_train[i]).float()\n",
    "            total_correct = torch.tensor(correct_pred.sum().item() / (len(correct_pred)))\n",
    "            total_correct = torch.round(total_correct * 100)\n",
    "            correct += total_correct\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "            if (i >= 5):\n",
    "                i = 0\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_pred = net(X_test)\n",
    "            total_test_loss = criterion(test_pred,y_test.long())\n",
    "            test_softmax = F.softmax(preds, dim = 1)\n",
    "            _, test_tags = torch.max(test_softmax, dim = 1)\n",
    "            correct_test_pred = (test_tags == y_test).float()\n",
    "            total_test_correct = torch.tensor(correct_test_pred.sum().item() / (len(correct_test_pred)))\n",
    "            total_test_correct = torch.round(total_test_correct * 100)\n",
    "        total_loss = total_loss/5\n",
    "        correct = correct/5\n",
    "        test_loss += total_test_loss.item()\n",
    "        test_correct += total_test_correct\n",
    "        train_correct += correct\n",
    "        train_loss += total_loss\n",
    "        \n",
    "    # Averaging train and test loss after all epochs for each dataset.\n",
    "    # Averaging train and test correct after all epochs for each dataset.\n",
    "    test_loss = test_loss/100\n",
    "    train_correct = train_correct/100\n",
    "    train_loss = train_loss/100\n",
    "    test_correct = test_correct/100\n",
    "    \n",
    "    # Appending to train_loss_list and test_loss_list after averaging for each dataset.\n",
    "    # Appending to train_correct_list and test_correct_list after averaging for each dataset.\n",
    "    test_loss_list.append(test_loss)\n",
    "    test_correct_list.append(test_correct)\n",
    "    train_correct_list.append(train_correct)\n",
    "    train_loss_list.append(train_loss)\n",
    "    full_loss += np.mean(train_loss_list)\n",
    "    full_correct += np.mean(train_correct_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
